* Introduction

Two new device-mapper targets are in development.  The first provides
thin provisioning at a large level of granularity (eg, several
megabytes of storage are provisioned at a time).  This target does not
allow sharing between thin-provisioned targets of the 'pool' of free
space.  This is a pragmatic solution, it is good enough for many
people's requirements, and simple to implement.

The second target is currently going by the name of 'multisnap'.  It
provides thin provisioning _and_ snapshot facilities.  It allows people
to use small granularity of space allocation, and share data and free
space between different instances of the target.


* Thin provisioning

** Problem description

  - Most volumes have unused space on them.
  - Unnecessary storage deployment raises costs.
  - Manual solution  <Describe manual process for reducing wasted space by extending lvs regularly>
    - Extend logical volume
    - Get application to extend (eg, filesystem or database).  If the
      application can't extend 'online' then you'll have to umount the
      volume for this step and incur a service outage.

** Solution

  - Drop the early provisioning paradigm of allocating all space at
    device creation time in favour of late provisioning which allocates on
    demand; this is comparible to virtual memory concepts.

  - Create large virtual devices, but only allocate when the application
    first writes to a particular area of the volume.

** Metadata

  - Since we allocate on demand, the mapping from logical blocks on
    the thinp device to the underlying backing store will not be a
    simple one.  It reflects the order that blocks were first written
    to.

  - Currently all metadata is stored by the user space volume manager
    (eg, LVM) and passed in via the dm-target contructor arguments.
    This solution is not suitable for thinp for a couple of reasons:

     i) There are a large number of mappings, potentially millions,
        which is just too much to pass on a target line.

     ii) In order to maintain good performance the target itself must
         allocate new mappings when needed.  This allocation decision
         must be robust and survive power failure etc.  So the target
         writes it's own metadata.

    Side note: The LVM2 metadata format, begin text based, does not
    scale nicely to such large mappings.  So a userland based metadata
    management would involve a profound change to the LVM2 metadata.

** Separate metadata and data volumes

  - It's slightly simpler if we put the metadata and data on different
    volumes.  For example, adding data space is easier.  We can have
    different block sizes for the data and metadata.

  - Metadata device hidden by LVM system so sys admin unlikely to be
    aware of it.

** Data volume

  - Space comes from a preallocated 'data volume', which is itself just
    another dm device.

  - Typically the data volume will only contain enough excess space
    for a few new mappings.  Userland can extend the data volume if
    the free space gets too low.

  - Different thin provisioned volumes may _not_ share the same pool.
    A deliberate decision to keep things simple.  Prompt extension of
    the pool will allow the excess space in the pool to be kept to a
    minimum.  (For those still want pool sharing see the multi-snapshot
    target.)

** Block size choice

  - A block is the unit of allocation.

  - The choice of block size can has a couple of trade offs:

    - The smaller the block size, the more potential for saving space.
      eg, a single byte write to an unallocated block will cause a
      whole blocks worth of disk to be allocated.

      <can we come up with some examples here?  eg, mkfs with small and large block sizes>

    - The larger the block size:
      - The less chance there is of fragmentation.  Where logically
        adjacent data ends up physically separated on the disk, thus
        incurring extra seek overhead.

      - The less frequently we need to allocate new mappings, and
        update the metadata.

      - The smaller the metadata tables.  The target preallocates a
        fixed amount of memory for storing mapping details.  A subset
        of the mapping table is held in core at any one time.  An IO
        that requires a mapping that isn't in the in core table will
        incur a disk read to load it.  (For this simple thinp target
        the whole table commonly fits in core.)

** Reading an unprovisioned block

  Sometimes an application will try and read an unprovisioned block.
  For example udev hunting for a disk label.  There are several ways
  to address this:

  i) Map the block, which potentially wastes space.

  ii) Return zeroes, until the block is mapped and then return
  whatever is on the mapped block.

  iii) Return zeroes, keep track of which blocks have returned zeroes
  in this way and actually zero these blocks when they are mapped.
  Adds metadata overhead and adds a lot of latency to the mapping
  operation.

  iv) Error the IO.  Antisocial.

  We've decided to go with option (ii), it gives the best performance
  and there is a precident with other hardware devices behaving
  similarly.

** Configuration/Use

  - Allocate (empty) logical volume for the thin provisioning pool
  - Allocate small logical volume for the thin provisioning metadata
  - Set up thin provisioning mapped device on aforementioned 2 LVs
  - Access as usual (mkfs, ...)
  - In case the pool runs full, lvextend the pool logical volumne
    accordingly
  - Reload thin provisioned mapped device in order to inform it
    about the size change

  LVM2 tools take care of all this for you.

** Performance

  - Expensive operation is mapping in a new 'block'.  Writing across a
    totally unprovisioned volume with a small block size (eg, 64k)
    within a few percentage points of native performance.  Larger
    block sizes are pretty much at native performance.

  - Once a block has been mapped access occurs at native device speed.

** Current status

  - Experimental device mapper target available.

  - LVM tools should follow shortly, making it easier for people to
    experiment.

  - Hopefully in mainline kernel within 6 months.


* Snapshots

** Existing snapshots

  - Take a writable snapshot of any volume (device mapper or otherwise).
  - snapshot shows the volume as it was when taken, origin can continue to be used.
  - 'writable' in that you may write to the snapshot and diverge from the origin.
  - Works by storing the differences from the origin device.  These
    differences are stored in a separate LV (backing volume).  Once
    snapshot per backing volume.
  - As more differences acrue you may have to resize the backing volume
  - The 'merge' operation allows you to discard any subsequent changes
    to the origin and instead use copy the snapshot contents back over
    the origin.  This can be done on a live system.

** Problems with existing snapshots

** Multisnaps

   describe frequent, regular snaps with dropping of
   intermediate snaps scenario.

** Metadata object

   Once we start sharing metadata between various thinly provisioned
   devices we end up with a separate 'object' in the kernel that needs
   several operations:

*** create/delete virtual device

    We intend to store large numbers of virtual devices in a single
    metadata area.  These devices will be created and deleted
    frequently.  We don't want to have to load a dm table for a
    virtual device in order to delete it.  So we communicate directly
    with the metadata object to do this.

*** New virtual device (possibly a snapshot)

    Similarly we may want to take a snapshot of an existing device
    without creating a dm device for the origin.  The origin can be
    an external device (ie. outside the metadata area), internal or
    non-existant (thinp).

*** set external transaction id

    Userland volume managers (ie. LVM) will use a transactional model
    for storing their metadata.  The 'external transaction id' allows
    a sort of two phase commit to occur, where the userland updates
    it's own metadata, then updates the metadata device and finally
    updates it's own metadata.  This allows userland to recover from
    crashes.

*** hold/drop transaction

    Some features will be provided by userland, either because
    they're too computationally expensive to put in the kernel, or
    just for separation of policy from implementation.

    Here are a couple of examples:

    - How much space will I recover if I delete a particular device?

     We're putting many devices into one pool in order to maximise
     sharing between devices.  So while a device may have 1000 mapped
     blocks, 999 of them may be shared by other devices, in which case
     deleting the device is only going to recover one block.

     Looking at the reference counts for each mapped block involves
     walking the whole metadata tree for the device.  With large
     devices this can take a lot of IO, so something best kept out of
     the kernel.

     Keeping track of this count for each device would involve an
     inverse mapping from blocks to devices, and would be very
     expensive when sharing was broken.  Thus ruining performance.

    - Snapshot merge

     Merging a set of snapshot changes back into an external origin
     also requires a complete walk of the metadata tree to discover the
     changes and schedule them to be made by a kernel deamon (kcopyd).

     The walking and scheduling aspect will be performed by a userland
     utility.


    So both the above scenarios require userland to be performing
    _read only_ operations on the metadata at the same time as the
    kernel is working on the metadata.

    Fine grain locking to coordinate userland/kernel access and
    updates to the metadata would be too expensive to the performance
    of dm targets.  So instead there will an pair of operations that
    take a read only clone (I'm avoiding the work snapshot here) of
    the current metadata tree.  The same sharing mechanisms that make
    shared snapshots efficient will hopefully make this inexpensive.

    Userland will be given the location of an alternative superblock
    for the metadata device.  This is the root of a tree of blocks
    referring to other blocks in a variety of data structures (btrees,
    space maps etc.).  Blocks will be shared with the 'live' version
    of the metadata, their reference counts may change as sharing is
    broken, but we know the blocks will never be updated.

    Of course userland only has a view of the metadata at a fixed
    point in time.  In some cases this is adequate, for example the
    'how much space will be release if I delete this device' scenario.
    The block count returned by this may be slightly out of date.  But
    this is no worse a race than, say, using 'ps' to find out the
    memory usage of a process.

    For snapshot merge there are two scenarios.

    i) The merge should reflect the state of the snapshot at the
       current time.  A 'static' update.

    ii) The merge should include on going updates to the snapshot.  A
       'live' update.  This will involve switching a device over from
       pointing at the snapshot to pointing to the updated origin.

    For 'live' snapshot merging the userland process will need to iterate
    round taking successive views of the metadata.  Hopefully there
    will be fewer changes to merge in successive iterations.  At some
    point the changes will be deemed small enough that the snapshot
    can be suspended, the dm table reloaded to point to the origin and
    the few changes made before resuming.

*** Kernel interface to metadata object

    How do we talk to the metadata object in the kernel?

    i) Write a totally new kernel interface/system call.  (No one seriously considering this).
    ii) Widen the dm-ioctl interface.
    iii) Use the dm facility that allows us to send text strings to specific targets.
    iv) sysfs or whatever fs interface is flavour of the month.

   Going for (iii) since we often need to introduce a wrapper dm device
   around the data store anyway.

*** dm targets

**** 'pool'

     This target ties together a shared metadata volume and a shared
     data volume.  IO is mapped linearly to the data volume.  It
     accepts dm 'messages' from userland to implement the operations
     described in previous sections (delete device etc.).

     pool <metadata volume> <data volume> <low water mark>

     If the first 4k of the metadata volume has been zeroed then an
     empty metadata device will be created.

     The 'low water mark' is an edge trigerred value.  When the free
     space in the data volume crosses it, dmeventd will be notified.
     It will check the status line for the pool and take the
     appropriate action (ie. extend the data device).

     FIXME: add detail about the messages

**** 'thin'

     All devices stored within a metadata object are instanced with
     this target.  Be they fully mapped devices, thin provisioned
     devices, internal snapshots or external snapshots.

     The target line:

     thin <pool object> <internal device id>

     The 'internal device id' is just a 64bit number.  It must
     correspond to the id used in the 'create virtual device' message
     to the pool.  If the device id is not present in the metadata
     area the target constructor will fail.

** Sharing

  - Here's a picture

     #+CAPTION: This is the caption for the next figure link (or table)
     #+LABEL:   fig: blah blah
     #+ATTR_LaTeX: width=5cm,angle=30
    [[./test-image.svg]]

* Virtual machines




